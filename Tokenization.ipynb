{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad79addb",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## Tokenization is at the heart of LLMs\n",
    "\n",
    "<small> Simple string processing can be difficult due to tokenization for LLM </small>\n",
    "\n",
    "<small> Sometimes LLMs are bad at artithmetic due to tokenization </small>\n",
    "\n",
    "<small> Earlier versions of gpt had issues with Python due to tokenization </small>\n",
    "\n",
    "<small> LLMs do worse for non english languages due to tokenization </small>\n",
    "\n",
    "<small> Using YAML over json due to tokenization </small>\n",
    "\n",
    "<small> tiktokenizer.vercel.app is good for visualizing tokenization </small>\n",
    "\n",
    "<small>So like even for the same concept \"egg\" can be very different tokens and ids. The model has to somehow learn that these are the same concept and group them in the nn properly. </small>\n",
    "<small> For GPT4, their tokenizer is able to group white spaces into one token which is great for its Python coding ability because of the fact that Python needs many spaces </small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0246edca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord(\"h\") #gets the unocode integer value of a character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4fd039b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nWe can't just use this for tokenization as many characters map to the same integer value and also not stable\\nUTF-8 takes every code point and encodes it into 1-4 bytes. Each byte is an integer from 0-255\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ord(x) for x in \"hello\"] #gets the unicode integer values of all characters in a string\\\n",
    "'''\n",
    "We can't just use this for tokenization as many characters map to the same integer value and also not stable\n",
    "UTF-8 takes every code point and encodes it into 1-4 bytes. Each byte is an integer from 0-255\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b6c0bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nthe other utf encodings are utf-16 and utf-32 and are pretty wasteful\\nWe want to be able to support larger vocabulary sizes so we use byte pair encoding (BPE)\\nBPE works by merging the most common pairs of bytes iteratively to form a vocabulary of tokens\\nThis way we can represent common words or subwords with a single token, while still being able\\nto represent rare words with multiple tokens.\\nTo want to just have raw bytes in there, you have to change the structure of the transformer,\\ntheres a research paper on it: MEGABYTE: Predicting Million-byte sequences with multiscale transformers\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(\"hello\".encode(\"utf-8\"))  # encodes the string into bytes using utf-8 encoding\n",
    "'''\n",
    "the other utf encodings are utf-16 and utf-32 and are pretty wasteful\n",
    "We want to be able to support larger vocabulary sizes so we use byte pair encoding (BPE)\n",
    "BPE works by merging the most common pairs of bytes iteratively to form a vocabulary of tokens\n",
    "This way we can represent common words or subwords with a single token, while still being able\n",
    "to represent rare words with multiple tokens.\n",
    "To want to just have raw bytes in there, you have to change the structure of the transformer,\n",
    "theres a research paper on it: MEGABYTE: Predicting Million-byte sequences with multiscale transformers\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6081e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "long sentence here: The quick brown fox jumps over the lazy dog.\n",
      "length:  64\n",
      "---\n",
      "[108, 111, 110, 103, 32, 115, 101, 110, 116, 101, 110, 99, 101, 32, 104, 101, 114, 101, 58, 32, 84, 104, 101, 32, 113, 117, 105, 99, 107, 32, 98, 114, 111, 119, 110, 32, 102, 111, 120, 32, 106, 117, 109, 112, 115, 32, 111, 118, 101, 114, 32, 116, 104, 101, 32, 108, 97, 122, 121, 32, 100, 111, 103, 46]\n",
      "length:  64\n"
     ]
    }
   ],
   "source": [
    "text = \"long sentence here: The quick brown fox jumps over the lazy dog.\"\n",
    "tokens = text.encode(\"utf-8\") # raw bytes\n",
    "tokens = list(map(int, tokens)) # convert bytes to integers\n",
    "print('---')\n",
    "print (text)\n",
    "print(\"length: \", len(text))\n",
    "print('---')\n",
    "print(tokens)\n",
    "print(\"length: \", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c281ca39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(108, 111): 1, (111, 110): 1, (110, 103): 1, (103, 32): 1, (32, 115): 1, (115, 101): 1, (101, 110): 2, (110, 116): 1, (116, 101): 1, (110, 99): 1, (99, 101): 1, (101, 32): 3, (32, 104): 1, (104, 101): 3, (101, 114): 2, (114, 101): 1, (101, 58): 1, (58, 32): 1, (32, 84): 1, (84, 104): 1, (32, 113): 1, (113, 117): 1, (117, 105): 1, (105, 99): 1, (99, 107): 1, (107, 32): 1, (32, 98): 1, (98, 114): 1, (114, 111): 1, (111, 119): 1, (119, 110): 1, (110, 32): 1, (32, 102): 1, (102, 111): 1, (111, 120): 1, (120, 32): 1, (32, 106): 1, (106, 117): 1, (117, 109): 1, (109, 112): 1, (112, 115): 1, (115, 32): 1, (32, 111): 1, (111, 118): 1, (118, 101): 1, (114, 32): 1, (32, 116): 1, (116, 104): 1, (32, 108): 1, (108, 97): 1, (97, 122): 1, (122, 121): 1, (121, 32): 1, (32, 100): 1, (100, 111): 1, (111, 103): 1, (103, 46): 1}\n",
      "[(3, (104, 101)), (3, (101, 32)), (2, (101, 114)), (2, (101, 110)), (1, (122, 121)), (1, (121, 32)), (1, (120, 32)), (1, (119, 110)), (1, (118, 101)), (1, (117, 109)), (1, (117, 105)), (1, (116, 104)), (1, (116, 101)), (1, (115, 101)), (1, (115, 32)), (1, (114, 111)), (1, (114, 101)), (1, (114, 32)), (1, (113, 117)), (1, (112, 115)), (1, (111, 120)), (1, (111, 119)), (1, (111, 118)), (1, (111, 110)), (1, (111, 103)), (1, (110, 116)), (1, (110, 103)), (1, (110, 99)), (1, (110, 32)), (1, (109, 112)), (1, (108, 111)), (1, (108, 97)), (1, (107, 32)), (1, (106, 117)), (1, (105, 99)), (1, (103, 46)), (1, (103, 32)), (1, (102, 111)), (1, (101, 58)), (1, (100, 111)), (1, (99, 107)), (1, (99, 101)), (1, (98, 114)), (1, (97, 122)), (1, (84, 104)), (1, (58, 32)), (1, (32, 116)), (1, (32, 115)), (1, (32, 113)), (1, (32, 111)), (1, (32, 108)), (1, (32, 106)), (1, (32, 104)), (1, (32, 102)), (1, (32, 100)), (1, (32, 98)), (1, (32, 84))]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nSo this gives us the frequency of each adjacent byte pair in the token list\\nIt wil be a dictionary where the keys are tuples of byte pairs and the values are their counts\\nWe can use this to find the most common byte pairs and merge them iteratively to form our BPE vocabulary\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]): #Pythonic way to iterate over adjacent elements\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "stats = get_stats(tokens)\n",
    "print(stats)\n",
    "print(sorted(((value, key) for key, value in stats.items()), reverse=True))  #print 10 most common byte pairs\n",
    "\n",
    "'''\n",
    "So this gives us the frequency of each adjacent byte pair in the token list\n",
    "It wil be a dictionary where the keys are tuples of byte pairs and the values are their counts\n",
    "We can use this to find the most common byte pairs and merge them iteratively to form our BPE vocabulary\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34f9cf1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "top_pair = max(stats, key=stats.get) # get the byte pair with the highest count\n",
    "top_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "321afff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5]\n",
      "[108]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "def merge(ids, pair, idx):\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        #If we are not at the very last position AND the pair matches, replace it \n",
    "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i + 1] == pair[1]:\n",
    "            newids.append(idx)  #append the new merged token\n",
    "            i += 2  #skip the next token as it's part of the merged pair\n",
    "        else:\n",
    "            newids.append(ids[i])  #append the current token as is\n",
    "            i += 1\n",
    "        return newids\n",
    "print(merge([5,6,6,7,9,1], (6,7), 99)) #example usage\n",
    "tokens = merge(tokens, top_pair, 256)  #merge the most common pair and assign it a new token id (256)\n",
    "print(tokens)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9962c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Now do it again, iteratively, the more steps we take, the larger our vocabulary \n",
    " becomes and shorter our token sequences\n",
    " ''' \n",
    "text = '''A Programmerâ€™s Introduction to Unicode March 3, 2017 Â· Coding Â· 25 Comments  ï¼µï½Žï½‰ï½ƒï½ï½„ï½…! ðŸ…¤ðŸ…ðŸ…˜ðŸ…’ðŸ…žðŸ…“ðŸ…”â€½ ðŸ‡ºâ€ŒðŸ‡³â€ŒðŸ‡®â€ŒðŸ‡¨â€ŒðŸ‡´â€ŒðŸ‡©â€ŒðŸ‡ª! ðŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception. A few months ago, I got interested in Unicode and decided to spend some time learning more about it in detail. In this article, Iâ€™ll give an introduction to it from a programmerâ€™s point of view. Iâ€™m going to focus on the character set and whatâ€™s involved in working with strings and files of Unicode text. However, in this article Iâ€™m not going to talk about fonts, text layout/shaping/rendering, or localization in detailâ€”those are separate issues, beyond my scope (and knowledge) here. Diversity and Inherent Complexity The Unicode Codespace Codespace Allocation Scripts Usage Frequency Encodings UTF-8 UTF-16 Combining Marks Canonical Equivalence Normalization Forms Grapheme Clusters And Moreâ€¦ Diversity and Inherent Complexity As soon as you start to study Unicode, it becomes clear that it represents a large jump in complexity over character sets like ASCII that you may be more familiar with. Itâ€™s not just that Unicode contains a much larger number of characters, although thatâ€™s part of it. Unicode also has a great deal of internal structure, features, and special cases, making it much more than what one might expect a mere â€œcharacter setâ€ to be. Weâ€™ll see some of that later in this article. Moreover, Unicode is committed not just to supporting texts in any single language, but also to letting multiple languages coexist within one textâ€”which introduces even more complexity. Most programming languages have libraries available to handle the gory low-level details of text manipulation, but as a programmer, youâ€™ll still need to know about certain Unicode features in order to know when and how to apply them. It may take some time to wrap your head around it all, but donâ€™t be discouragedâ€”think about the billions of people for whom your software will be more accessible through supporting text in their language. Embrace the complexity! The Unicode Codespace Letâ€™s start with some general orientation. The basic elements of Unicodeâ€”its â€œcharactersâ€, although that term isnâ€™t quite rightâ€”are called code points. Code points are identified by number, customarily written in hexadecimal with the prefix â€œU+â€, such as U+0041 â€œAâ€ latin capital letter a or U+03B8 â€œÎ¸â€ greek small letter theta. Each code point also has a short name, and quite a few other properties, specified in the Unicode Character Database. The set of all possible code points is called the codespace. The Unicode codespace consists of 1,114,112 code points. However, only 128,237 of themâ€”about 12% of the codespaceâ€”are actually assigned, to date. Thereâ€™s plenty of room for growth! Unicode also reserves an additional 137,468 code points as â€œprivate useâ€ areas, which have no standardized meaning and are available for individual applications to define for their own purposes. Codespace Allocation To get a feel for how the codespace is laid out, itâ€™s helpful to visualize it. Below is a map of the entire codespace, with one pixel per code point. Itâ€™s arranged in tiles for visual coherence; each small square is 16Ã—16 = 256 code points, and each large square is a â€œplaneâ€ of 65,536 code points. There are 17 planes altogether. White represents unassigned space. Blue is assigned code points, green is private-use areas, and the small red area is surrogates (more about those later). As you can see, the assigned code points are distributed somewhat sparsely, but concentrated in the first three planes. Plane 0 is also known as the â€œBasic Multilingual Planeâ€, or BMP. The BMP contains essentially all the characters needed for modern text in any script, including Latin, Cyrillic, Greek, Han (Chinese), Japanese, Korean, Arabic, Hebrew, Devanagari (Indian), and many more. (In the past, the codespace was just the BMP and no moreâ€”Unicode was originally conceived as a straightforward 16-bit encoding, with only 65,536 code points. It was expanded to its current size in 1996. However, the vast majority of code points in modern text belong to the BMP.) Plane 1 contains historical scripts, such as Sumerian cuneiform and Egyptian hieroglyphs, as well as emoji and various other symbols. Plane 2 contains a large block of less-common and historical Han characters. The remaining planes are empty, except for a small number of rarely-used formatting characters in Plane 14; planes 15â€“16 are reserved entirely for private use. Scripts Letâ€™s zoom in on the first three planes, since thatâ€™s where the action is: Map of scripts in Unicode planes 0â€“2 (click to zoom) This map color-codes the 135 different scripts in Unicode. You can see how Han () and Korean () take up most of the range of the BMP (the left large square). By contrast, all of the European, Middle Eastern, and South Asian scripts fit into the first row of the BMP in this diagram. Many areas of the codespace are adapted or copied from earlier encodings. For example, the first 128 code points of Unicode are just a copy of ASCII. This has clear benefits for compatibilityâ€”itâ€™s easy to losslessly convert texts from smaller encodings into Unicode (and the other direction too, as long as no characters outside the smaller encoding are used). Usage Frequency One more interesting way to visualize the codespace is to look at the distribution of usageâ€”in other words, how often each code point is actually used in real-world texts. Below is a heat map of planes 0â€“2 based on a large sample of text from Wikipedia and Twitter (all languages). Frequency increases from black (never seen) through red and yellow to white. You can see that the vast majority of this text sample lies in the BMP, with only scattered usage of code points from planes 1â€“2. The biggest exception is emoji, which show up here as the several bright squares in the bottom row of plane 1. Encodings Weâ€™ve seen that Unicode code points are abstractly identified by their index in the codespace, ranging from U+0000 to U+10FFFF. But how do code points get represented as bytes, in memory or in a file? The most convenient, computer-friendliest (and programmer-friendliest) thing to do would be to just store the code point index as a 32-bit integer. This works, but it consumes 4 bytes per code point, which is sort of a lot. Using 32-bit ints for Unicode will cost you a bunch of extra storage, memory, and performance in bandwidth-bound scenarios, if you work with a lot of text. Consequently, there are several more-compact encodings for Unicode. The 32-bit integer encoding is officially called UTF-32 (UTF = â€œUnicode Transformation Formatâ€), but itâ€™s rarely used for storage. At most, it comes up sometimes as a temporary internal representation, for examining or operating on the code points in a string. Much more commonly, youâ€™ll see Unicode text encoded as either UTF-8 or UTF-16. These are both variable-length encodings, made up of 8-bit or 16-bit units, respectively. In these schemes, code points with smaller index values take up fewer bytes, which saves a lot of memory for typical texts. The trade-off is that processing UTF-8/16 texts is more programmatically involved, and likely slower. UTF-8 In UTF-8, each code point is stored using 1 to 4 bytes, based on its index value. UTF-8 uses a system of binary prefixes, in which the high bits of each byte mark whether itâ€™s a single byte, the beginning of a multi-byte sequence, or a continuation byte; the remaining bits, concatenated, give the code point index. This table shows how it works: UTF-8 (binary)\tCode point (binary)\tRange 0xxxxxxx\txxxxxxx\tU+0000â€“U+007F 110xxxxx 10yyyyyy\txxxxxyyyyyy\tU+0080â€“U+07FF 1110xxxx 10yyyyyy 10zzzzzz\txxxxyyyyyyzzzzzz\tU+0800â€“U+FFFF 11110xxx 10yyyyyy 10zzzzzz 10wwwwww\txxxyyyyyyzzzzzzwwwwww\tU+10000â€“U+10FFFF A handy property of UTF-8 is that code points below 128 (ASCII characters) are encoded as single bytes, and all non-ASCII code points are encoded using sequences of bytes 128â€“255. This has a couple of nice consequences. First, any strings or files out there that are already in ASCII can also be interpreted as UTF-8 without any conversion. Second, lots of widely-used string programming idiomsâ€”such as null termination, or delimiters (newlines, tabs, commas, slashes, etc.)â€”will just work on UTF-8 strings. ASCII bytes never occur inside the encoding of non-ASCII code points, so searching byte-wise for a null terminator or a delimiter will do the right thing. Thanks to this convenience, itâ€™s relatively simple to extend legacy ASCII programs and APIs to handle UTF-8 strings. UTF-8 is very widely used in the Unix/Linux and Web worlds, and many programmers argue UTF-8 should be the default encoding everywhere. However, UTF-8 isnâ€™t a drop-in replacement for ASCII strings in all respects. For instance, code that iterates over the â€œcharactersâ€ in a string will need to decode UTF-8 and iterate over code points (or maybe grapheme clustersâ€”more about those later), not bytes. When you measure the â€œlengthâ€ of a string, youâ€™ll need to think about whether you want the length in bytes, the length in code points, the width of the text when rendered, or something else. UTF-16 The other encoding that youâ€™re likely to encounter is UTF-16. It uses 16-bit words, with each code point stored as either 1 or 2 words. Like UTF-8, we can express the UTF-16 encoding rules in the form of binary prefixes: UTF-16 (binary)\txxxxxxxxxxxxxxxx\tCode point (binary)\tRange xxxxxxxxxxxxxxxx\tU+0000â€“U+FFFF 110110xxxxxxxxxx 110111yyyyyyyyyy\txxxxxxxxxxyyyyyyyyyy + 0x10000\tU+10000â€“U+10FFFF A more common way that people talk about UTF-16 encoding, though, is in terms of code points called â€œsurrogatesâ€. All the code points in the range U+D800â€“U+DFFFâ€”or in other words, the code points that match the binary prefixes 110110 and 110111 in the table aboveâ€”are reserved specifically for UTF-16 encoding, and donâ€™t represent any valid characters on their own. Theyâ€™re only meant to occur in the 2-word encoding pattern above, which is called a â€œsurrogate pairâ€. Surrogate code points are illegal in any other context! Theyâ€™re not allowed in UTF-8 or UTF-32 at all. Historically, UTF-16 is a descendant of the original, pre-1996 versions of Unicode, in which there were only 65,536 code points. The original intention was that there would be no different â€œencodingsâ€; Unicode was supposed to be a straightforward 16-bit character set. Later, the codespace was expanded to make room for a long tail of less-common (but still important) Han characters, which the Unicode designers didnâ€™t originally plan for. Surrogates were then introduced, asâ€”to put it bluntlyâ€”a kludge, allowing 16-bit encodings to access the new code points. Today, Javascript uses UTF-16 as its standard string representation: if you ask for the length of a string, or iterate over it, etc., the result will be in UTF-16 words, with any code points outside the BMP expressed as surrogate pairs. UTF-16 is also used by the Microsoft Win32 APIs; though Win32 supports either 8-bit or 16-bit strings, the 8-bit version unaccountably still doesnâ€™t support UTF-8â€”only legacy code-page encodings, like ANSI. This leaves UTF-16 as the only way to get proper Unicode support in Windows. (Update: in Win10 version 1903, they finally added UTF-8 support to the 8-bit APIs! ðŸ˜Š) Combining Marks In the story so far, weâ€™ve been focusing on code points. But in Unicode, a â€œcharacterâ€ can be more complicated than just an individual code point! Unicode includes a system for dynamically composing characters, by combining multiple code points together. This is used in various ways to gain flexibility without causing a huge combinatorial explosion in the number of code points. In European languages, for example, this shows up in the application of diacritics to letters. Unicode supports a wide range of diacritics, including acute and grave accents, umlauts, cedillas, and many more. All these diacritics can be applied to any letter of any alphabetâ€”and in fact, multiple diacritics can be used on a single letter. If Unicode tried to assign a distinct code point to every possible combination of letter and diacritics, things would rapidly get out of hand. Instead, the dynamic composition system enables you to construct the character you want, by starting with a base code point (the letter) and appending additional code points, called combining marks, to specify the diacritics. When a text renderer sees a sequence like this in a string, it automatically stacks the diacritics over or under the base letter to create a composed character. For example, the accented character â€œÃâ€ can be expressed as a string of two code points: U+0041 â€œAâ€ latin capital letter a plus U+0301 â€œâ—ŒÌâ€ combining acute accent. This string automatically gets rendered as a single character: â€œAÌâ€. In practice, there are precomposed code points for most of the common letter-with-diacritic combinations in European-script languages, so they donâ€™t use dynamic composition that much in typical text. Still, the system of combining marks does allow for an arbitrary number of diacritics to be stacked on any base character. The reductio-ad-absurdum of this is Zalgo text, which works by ÍŸÍ…Í–rÍžanÌ­Ì«Ì Ì–ÍˆÌ—dÍ–Ì»Ì¹oÍmÌªÍ™Í•Ì—ÌlÌ§Í‡Ì°Í“Ì³Ì«yÍÍ“Ì¥ÌŸÍ Ì•sÌ«tÍœÌ«Ì±Í•Ì—Ì°Ì¼Ì˜aÍÌ¼Ì©Í–Í‡Ì ÍˆÌ£cÌ™ÍkÍ˜Ì–Ì±Ì¹ÍiÌ¢nÌ¨ÌºÌÍ‡Ì‡ÌŸÍ™gÌ§Ì«Ì®ÍŽÍ…Ì»ÌŸ Ì•nÍžÌ¼ÌºÍˆuÌ®Í™mÍžÌºÌ­ÌŸÌ—eÌžÍ“Ì°Ì¤Í“Ì«rÌµoÌ–uÌ­sÒ‰ÌªÍÌ­Ì¬ÌÌ¤ Í Ì®Í‰ÌÌžÌ—ÌŸdÌ´ÌŸÌœÌ±Í•ÍšiÍ¡Í‡Ì«Ì¼Ì¯Ì­ÌœaÌ¥Í™Ì»Ì¼cÌ²Ì²Ì¹rÌ¨Ì Ì¹Ì£Ì°Ì¦iÌ±tÌ•Ì¤Ì»Ì¤ÍÍ™Ì˜iÌµÌœÌ­Ì¤Ì±ÍŽcÌµÌ¦oÍ¢Ì°Ì¥Í“Ì£Ì«Ì™Ì¤tÌžÌžÌ‡Ì²Í‰Ì²Ì» Ì•Ì³ÌªÌ Í–Ì³Ì¯aÍœÌ«nÍÌ¼dÍ¡ Ì£Ì¦Í…Ì™cÌªÌ—rÌ´Í™Ì®Ì¦Ì¹Ì³eÍŸÍ‡ÍšÌžÍ”Ì¹Ì«aÌ™ÌºÌ™tÌ¦Í”ÍŽÍ…Ì˜Ì¹eÌ¥Ì©Í aÍ–ÌªÌœÌ®Í™Ì¹nÌ¢Í‰Ì ÍÍ‡Í‰Í“Ì¦Ì¼aÌ³Í–ÌªÌ¤Ì±pÍ Ì–Í”Í”ÌŸÍ‡ÍŽpÌ±ÍÌºeÌ¨Ì²ÍŽÍˆÌ°Ì²Ì¤Ì«aÍœÌ¯rÌ¨Ì®Ì«Ì£Ì˜aÌ©Ì¯Í–nÌ¹Ì¦Ì°ÍŽÌ£ÌžÌžcÌ¨Ì¦Ì±Í”ÍŽÍÍ–eÍ˜Ì¬Í“ Ì¤Ì°Ì©Í™Ì¤Ì¬Í™oÌµÌ¼Ì»Ì¬Ì»Í‡Ì®ÌªfÌ´ Ì¡Ì™Ì­Í“Í–ÌªÌ¤â€œÌ¸Í™Ì Ì¼cÍœÌ³Ì—oÍÌ¼Í™Í”Ì®rÌžÌ«ÌºÌžÌ¥Ì¬ruÌºÌ»Ì¯Í‰Ì­Ì»Ì¯pÍ¢Ì°Ì¥Í“Ì£Ì«Ì™Ì¤tÍ…Ì³ÍÌ³Ì–iÌ¶ÍˆÌÍ™Ì¼Ì™Ì¹oÌ¡Í”nÍÍ…Ì™ÌºÌ¹Ì–Ì©â€Ì¨Ì—Í–ÍšÌ©.Ì¯Í“ Further reading: The Unicode Standard UTF-8 Everywhere Manifesto Dark corners of Unicode by Eevee ICU (International Components for Unicode)â€”C/C++/Java libraries implementing many Unicode algorithms and related things Python 3 Unicode Howto Google Noto Fontsâ€”set of fonts intended to cover all assigned code points The Many Meanings of â€œShaderâ€Quadrilateral Interpolation, Part 2 25 Comments on â€œA Programmerâ€™s Introduction to Unicodeâ€ Texture Gathers and Coordinate Precision git-partial-submodule Slope Space in BRDF Theory Hash Functions for GPU Rendering All Posts Categories Graphics(32) Coding(23) Math(21) GPU(15) Physics(6) Eye Candy(4) Â© 2007â€“2025 by Nathan Reed. Licensed CC-BY-4.0.'''\n",
    "tokens = text.encode(\"utf-8\") # raw bytes\n",
    "tokens = list(map(int, tokens)) # convert bytes to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c34acde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging (101, 32) into a new token 256\n",
      "merging (105, 110) into a new token 257\n",
      "merging (115, 32) into a new token 258\n",
      "merging (116, 104) into a new token 259\n",
      "merging (99, 111) into a new token 260\n",
      "merging (116, 32) into a new token 261\n",
      "merging (101, 114) into a new token 262\n",
      "merging (97, 110) into a new token 263\n",
      "merging (44, 32) into a new token 264\n",
      "merging (111, 114) into a new token 265\n",
      "merging (100, 32) into a new token 266\n",
      "merging (226, 128) into a new token 267\n",
      "merging (260, 100) into a new token 268\n",
      "merging (97, 114) into a new token 269\n",
      "merging (101, 110) into a new token 270\n",
      "merging (121, 32) into a new token 271\n",
      "merging (46, 32) into a new token 272\n",
      "merging (257, 103) into a new token 273\n",
      "merging (97, 108) into a new token 274\n",
      "merging (259, 256) into a new token 275\n"
     ]
    }
   ],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]): #Pythonic way to iterate over adjacent elements\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "def merge(ids, pair, idx):\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        #If we are not at the very last position AND the pair matches, replace it \n",
    "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i + 1] == pair[1]:\n",
    "            newids.append(idx)  #append the new merged token\n",
    "            i += 2  #skip the next token as it's part of the merged pair\n",
    "        else:\n",
    "            newids.append(ids[i])  #append the current token as is\n",
    "            i += 1\n",
    "    return newids\n",
    "# ---\n",
    "vocab_size = 276 #The desired final vocabulary size\n",
    "num_merges = vocab_size - 256  #Initial byte vocab size is 256\n",
    "ids = list(tokens)  #Make a copy of the original token list\n",
    "\n",
    "merges = {} #(int, int) -> int\n",
    "for i in range(num_merges):\n",
    "    stats = get_stats(ids)\n",
    "    pair = max(stats, key=stats.get) # get the byte pair with the highest count\n",
    "    idx = 256 + i  #New token index\n",
    "    merges[pair] = idx\n",
    "    print(f\"merging {pair} into a new token {idx}\")\n",
    "    ids = merge(ids, pair, idx)  #merge the most common pair and assign it a new token id\n",
    "    merges[pair] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5f7b6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens length: 15437\n",
      "ids length: 12200\n",
      "compression ratio: 1.27X\n"
     ]
    }
   ],
   "source": [
    "print(\"tokens length:\", len(tokens))\n",
    "print(\"ids length:\", len(ids))\n",
    "print(\"compression ratio:\", f\"{len(tokens)/len(ids):.2f}X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83534ace",
   "metadata": {},
   "source": [
    "<small> Note: Tokenizer is a completely separate, independent module from the LLM. Has its own trainign set of text,\n",
    "which can be different from the LLM, on which you train the vocabulary using the BPE. It then translates back \n",
    "and forth between raw text and sequences of tokens. The LLM later only ever sees the tokens and doesnt directly\n",
    "deal with the text. </small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0677a8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï¿½\n"
     ]
    }
   ],
   "source": [
    "#Now at decoding\n",
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "for (p0, p1), idx in merges.items():\n",
    "    vocab[idx] = vocab[p0] + vocab[p1] #good thing we are using modern Python, where the dict preserves insertion order\n",
    "\n",
    "def decode(ids):\n",
    "    #given ids (list of integers), return Python string\n",
    "    tokens = b\"\".join(vocab[idx] for idx in ids) #b\"\" is one way to make a bytes object\"\n",
    "    text = tokens.decode(\"utf-8\", errors=\"replace\")  \n",
    "    return text\n",
    "\n",
    "print(decode([128])) #utf-8 cannot decode byte 128 on its own because theres a specific schema for multi-byte objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f32b7059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104, 101, 108, 108, 111, 32, 119, 265, 108, 100, 33]\n"
     ]
    }
   ],
   "source": [
    "def encode(text):\n",
    "    #given a string, return list of integers (the tokens)\n",
    "    tokens = list(text.encode(\"utf-8\"))\n",
    "    while len(tokens) >= 2:\n",
    "        stats = get_stats(tokens)\n",
    "        pair = min(stats, key=lambda p: merges.get(p, float('inf')))  #find the first pair in merges\n",
    "        '''\n",
    "        Function might fail if there is nothing to merge anymore\n",
    "\n",
    "        '''\n",
    "        if pair not in merges:\n",
    "            break #nothing else can be merged\n",
    "        idx = merges[pair]\n",
    "        tokens = merge(tokens, pair, idx)\n",
    "    return tokens\n",
    "        \n",
    "print(encode(\"hello world!\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d2f67715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world!\n"
     ]
    }
   ],
   "source": [
    "print(decode(encode(\"hello world!\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405b3789",
   "metadata": {},
   "source": [
    "<small> Merged splits with regex patterns (GPT series) </small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a6d68e",
   "metadata": {},
   "source": [
    "<small> So the issue is that you are allowing for different combinations of basically the same word and merging them together like dog, Dog, dog!. So this is considered suboptimal. For GPT-2, the used import regex as re, which is actually an extension of re. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9043e470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', \"'ve\", ' world', '123', ' HOW', \"'\", 'S', ' are', '      ', ' you', '!!!?']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThese are going to be conconated to form tokens in GPT-2 style tokenization\\nyou are never going to be merging the across the boundaries of these tokens\\nthis is how regex makes it so some merges cannot happen\\ngpt-2 did not use ignoreUppercase, so it brings issues when tokenizing\\n'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import regex as re\n",
    "gpt2pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?[0-9]+| ?[^\\s\\w]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "#The vertical barls are or's, you go left to right from this pattern and \n",
    "# try to match it against the string where you are\n",
    "\n",
    "print(re.findall(gpt2pat, \"Hello've world123 HOW'S are       you!!!?\"))\n",
    "'''\n",
    "These are going to be conconated to form tokens in GPT-2 style tokenization\n",
    "you are never going to be merging the across the boundaries of these tokens\n",
    "this is how regex makes it so some merges cannot happen\n",
    "gpt-2 did not use ignoreUppercase, so it brings issues when tokenizing\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d4a129d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[220, 220, 18435, 995, 3548, 3228, 30]\n",
      "[256, 22691, 1917, 7801, 3001, 30]\n",
      "[256, 22691, 1917, 7801, 3001, 30]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken as tk\n",
    "#GPT-2 (does not merge spaces)\n",
    "enc = tk.get_encoding(\"gpt2\")\n",
    "print(enc.encode(\"   Hello world??!!?\"))\n",
    "\n",
    "#GPT-4 (merges spaces)\n",
    "enc = tk.get_encoding(\"cl100k_base\")\n",
    "print(enc.encode(\"   Hello world??!!?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "21f3c3f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x81 in position 936: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[68]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mencoder.json\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     encoder = \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mapurv\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mDownloads\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mApurva downloads\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mPython\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mGPT-Tokenizer\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mvocab.bpe\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m, encoding=\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      6\u001b[39m     bpe_data = f.read()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\apurv\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\__init__.py:293\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(fp, *, \u001b[38;5;28mcls\u001b[39m=\u001b[38;5;28;01mNone\u001b[39;00m, object_hook=\u001b[38;5;28;01mNone\u001b[39;00m, parse_float=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    275\u001b[39m         parse_int=\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant=\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook=\u001b[38;5;28;01mNone\u001b[39;00m, **kw):\n\u001b[32m    276\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[32m    277\u001b[39m \u001b[33;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[32m    278\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    291\u001b[39m \u001b[33;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[32m    292\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loads(\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    294\u001b[39m         \u001b[38;5;28mcls\u001b[39m=\u001b[38;5;28mcls\u001b[39m, object_hook=object_hook,\n\u001b[32m    295\u001b[39m         parse_float=parse_float, parse_int=parse_int,\n\u001b[32m    296\u001b[39m         parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\apurv\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\encodings\\cp1252.py:23\u001b[39m, in \u001b[36mIncrementalDecoder.decode\u001b[39m\u001b[34m(self, input, final)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcharmap_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdecoding_table\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'charmap' codec can't decode byte 0x81 in position 936: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "\n",
    "with open('encoder.json', 'r') as f:\n",
    "    encoder = json.load(f)\n",
    "with open('C:\\\\Users\\\\apurv\\\\Downloads\\\\Apurva downloads\\\\Python\\\\GPT-Tokenizer\\\\vocab.bpe', 'r', encoding='utf-8') as f:\n",
    "    bpe_data = f.read()\n",
    "bpe_merges = [tuple(line.split()) for line in bpe_data.split('\\n')[1:-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1025a05",
   "metadata": {},
   "source": [
    "special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fb59e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mencoder\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'encoder' is not defined"
     ]
    }
   ],
   "source": [
    "len(encoder) #256 raw byte tokens. 50,000 merges. +1 special token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855fadf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
